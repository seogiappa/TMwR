# R 모델링 기초 검토 {#base-r}

```{r base-r-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
data(crickets, package = "modeldata")
library(tidyverse)
```

이 책은 R을 사용하여 모델을 생성하는 방법에 관한 것입니다. 깔끔한 데이터 원칙을 적용하는 방법에 대해 설명하기 전에, core R 언어(종종 'base R'이라고 함)에서 모델이 생성되고, 훈련되고, 사용되는 방법을 검토해 보겠습니다. 이 장은 핵심 언어 규칙에 대한 간략한 설명입니다. 완전하지는 않지만 독자(특히 R을 처음 접하는 독자)에게 가장 일반적으로 사용되는 기본적인 모티브를 제공합니다.

R이 기반이 되는 S 언어는 @WhiteBook(일반적으로 White Book으로 알려짐) 발행 이후 풍부한 데이터 분석 환경을 갖추고 있습니다. 이 버전의 S는 기호 모델 공식, 모델 행렬 및 데이터 프레임과 같은 오늘날 R 사용자에게 친숙한 표준 인프라 구성 요소와 데이터 분석을 위한 표준 객체 지향 프로그래밍 방법을 도입했습니다. 이러한 사용자 인터페이스는 그 이후에도 실질적으로 변경되지 않았습니다.

## 예

이러한 기본 사항을 설명하기 위해 @mangiafico2015를 통해 @mcdonald2009의 실험 데이터를 사용하여 주변 온도와 분당 귀뚜라미 울음소리 비율(chirp rate)의 관계를 살펴보겠습니다. 두 종에 대한 데이터가 수집되었습니다.: _O. exclamationis_ and _O. niveus_. 데이터는 총 `r nrow(crickets)`개의 데이터 포인트를 가진 `crickets`라는 데이터 프레임에 포함되어 있습니다. 이 데이터는 `r pkg(ggplot2)` 그래프에 표시됩니다.

```{r base-r-cricket-plot, out.width = '70%', fig.width=6, fig.height=4, warning = FALSE}
library(tidyverse)

data(crickets, package = "modeldata")
names(crickets)

# x축에는 온도, y축에는 귀뚜라미 울음 소리 비율(chirp rate)을 표시합니다. 
# 그래프의 요소는 각 종에 따라 서로 다른 색상이 지정됩니다.
ggplot(crickets, aes(x = temp, y = rate, col = species)) + 
  # 각 데이터에 대한 포인트 및 종별 색상 그리기
  geom_point() + 
  # 각 종에 대해 별도로 생성된 단순 선형 모델 적합 표시:
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Temperature (C)", y = "Chirp Rate (per minute)")
```
 
데이터는 각 종에 대해 상당히 선형적인 경향을 나타냅니다. 주어진 온도에 대해 _O. exclamationis_ 는 다른 종보다 분당 더 많이 짹짹거리는 것처럼 보입니다. 추론 모델의 경우 연구원은 데이터를 보기 전에 다음과 같은 귀무 가설(null hypotheses)을 지정했을 수 있습니다.:

 * 온도는 chirp rate에 영향을 미치지 않습니다.

 * 종의 chirp rate 사이에는 차이가 없습니다.

chirp rate을 예측하는 데에는 과학적이거나 실용적인 가치가 있을 수 있지만 이 예에서는 추론에 초점을 맞출 것입니다.

일반 선형 모형을 R에 적합시키기 위해 `lm()` 함수가 일반적으로 사용됩니다. 이 함수에 대한 중요한 인수는 모델 공식과 데이터를 포함하는 데이터 프레임입니다. 식은 _상징적_ 입니다. 예를 들어, 간단한 수식은 다음과 같습니다.:

```r
rate ~ temp
```
chirp rate이 결과이고(물결표 `~`의 왼쪽에 있기 때문에) 예측 변수임을 지정합니다.^[대부분의 모델 함수는 절편(intercept) 열을 내재적으로 추가합니다.] 측정값을 얻은 시간이 `time`이라는 열에 포함되어 있는 데이터를 가정해 보십시오. 수식은 :

```r
rate ~ temp + time
```

시간과 온도 값을 함께 추가하지 않습니다. 이 수식은 온도와 시간을 모델에 별도의 _주효과_ 로 추가해야 함을 상징적으로 나타냅니다. 주효과는 단일 예측 변수를 포함하는 모델 항입니다.

이 데이터에는 시간 측정이 없지만 같은 방식으로 종을 모델에 추가할 수 있습니다.

```r
rate ~ temp + species
```

종은 양적 변수가 아닙니다.; 데이터 프레임에서 `'O. exclamationis'` 및 `'O. niveus'` 수준의 요인 열로 표시됩니다. 대부분의 모델 함수는 숫자가 아닌 데이터에서는 작동할 수 없습니다. 따라서 종의 경우 모델은 종 데이터를 숫자 형식으로 _인코딩_ 해야 합니다. 가장 일반적인 접근 방식은 원래의 정성적 값 대신 지표 변수('더미 변수'라고도 함)를 사용하는 것입니다. 이 경우 종에는 두 가지 가능한 값이 있기 때문에 모델 수식은 종이 `"O. exclamationis"`일 때 값이 0이고, `"O. niveus"` 일 때 1의 값을 새 열을 추가하여 숫자로 자동 인코딩합니다. 기본 수식 기계는 모델을 생성하는 데 사용된 데이터 세트와 새 데이터 요소(예: 모델이 예측에 사용되는 경우)에 대해 이러한 값을 자동으로 변환합니다.

:::rmdnote
2개가 아닌 5개의 종이 있다고 가정합시다. 모델 공식은 4개의 종에 대한 이진 표시기인 _4개_ 의 추가 이진 열을 자동으로 추가합니다. 요인(즉, 첫 번째 수준)의 _참조 수준_ 은 항상 예측 변수 집합에서 제외됩니다. 네 개의 표시기 변수의 값을 알면 종의 값을 결정할 수 있다는 아이디어입니다. 이진 표시기 변수에 대해서는 \@ref(dummies) 섹션에서 더 자세히 논의합니다.
:::

위에 표시된 모델 수식은 각 종에 대해 다른 y-절편을 가진 모델을 생성합니다. 회귀선의 기울기도 종마다 다를 수 있습니다. 이 구조를 수용하기 위해 _interaction_ 용어를 모델에 추가할 수 있습니다. 이것은 몇 가지 다른 방법으로 지정할 수 있으며 가장 기본적인 것은 콜론을 사용합니다.:

```r
rate ~ temp + species + temp:species

# 손쉬운 방법을 사용하여 두 변수와의 상호 작용을 포함하는 모든 상호 작용을 확장할 수 있습니다.:
rate ~ (temp + species)^2

# 가능한 모든 상호 작용을 포함하도록 요인을 확장하는 또 다른 손쉬운 방법(이 예에 해당함)
rate ~ temp * species
```

표시기 변수를 자동으로 생성하는 편리함 외에도 수식은 몇 가지 다른 장점을 제공합니다.

* _In-line_ 함수를 수식에 사용할 수 있습니다. 예를 들어, 온도의 자연 로그를 사용하려면 `rate ~ log(temp)` 수식을 만들 수 있습니다. 수식은 기본적으로 기호이므로, 문자 그대로 수학은 항등 함수`I()`를 사용하는 예측 변수에도 적용될 수 있습니다. 화씨 단위를 사용하려면 섭씨에서 변환하기 위해 수식 `rate ~ I( (temp * 9/5) + 32 )`가 됩니다.

* R에는 수식 내에서 유용한 많은 함수가 있습니다. 예를 들어, `poly(x, 3)`는 모델에 대해서 `x`에 대한 선형, 2차 및 3차 항을 주효과로 생성합니다. `r pkg(splines)` 패키지에는 수식에서 비선형 스플라인 항을 생성하는 여러 기능도 가지고 있습니다.

* 예측 변수가 많은 데이터 세트의 경우, 마침표(.)를 사용할 수 있습니다. 마침표는 물결표의 왼쪽에 없는 모든 열에 대한 주효과를 나타냅니다. `~ (.)^3`을 사용하면 모델에 대한 모든 2개 및 3개 변수 상호작용뿐만 아니라 주효과를 생성할 수 있습니다.

귀뚜라미로 돌아가서 양방향 상호 작용 모델을 사용하겠습니다. 이 책에서는 피팅된 모델인 R 객체에 접미사 `_fit`을 사용합니다.

Returning to our chirping crickets, let's use a two-way interaction model. In this book, we use the suffix `_fit` for R objects that are fitted models. 

```{r base-r-cricket-fit}
interaction_fit <-  lm(rate ~ (temp + species)^2, data = crickets) 

# 모델의 간단한 요약을 인쇄하려면:
interaction_fit
```

이 출력은 읽기가 조금 어렵습니다. 종 표시기 변수의 경우 R은 구분 기호 없이 변수 이름(`species`)과 요인 수준(`O. niveus`)을 매시합니다.

이 모델에 대한 추론 결과로 들어가기 전에, 진단 플롯을 사용하여 피팅을 평가해야 합니다. `lm` 객체에 `plot()` 메소드를 사용할 수 있습니다. 이 메소드는 개체에 대한 4개의 플롯 세트를 생성하며, 각각은 피팅의 다른 측면을 보여줍니다. 두 개의 플롯이 여기에 표시됩니다.:

```{r interaction-plots, out.width = '100%', fig.width=8, fig.height=4.5, warning = FALSE}
# 두 개의 플롯을 나란히 배치:
par(mfrow = c(1, 2))

# 잔차 대 예측 값 표시:
plot(interaction_fit, which = 1)

# 잔차에 대한 정규 분위수 플롯:
plot(interaction_fit, which = 2)
```

이는 추론적 분석을 수행하기에 충분히 합리적으로 보입니다. 

:::rmdnote
표현식 평가의 기술적인 세부 사항에 관한 한,  R은 _게으릅니다_(열망하는 것과는 대조적으로). 이는 모델 피팅 함수가 일반적으로 가능한 마지막 순간에, 가능한 최소 수량을 계산한다는 것을 의미합니다. 예를 들어, 각 모델 항에 대한 계수 테이블에 관심이 있는 경우 모델과 함께 자동으로 계산되지 않고 대신 `summary()` 메서드를 통해 계산됩니다.
When it comes to the technical details of evaluating expressions, R is _lazy_ (as opposed to eager). This means that model fitting functions typically compute the minimum possible quantities at the last possible moment. For example, if you are interested in the coefficient table for each model term, this is not automatically computed with the model but is instead computed via the `summary()` method. 
:::

Our next order of business with the crickets is to assess if the inclusion of the interaction term is necessary. The most appropriate approach for this model is to re-compute the model without the interaction term and use the `anova()` method. 

```{r base-r-cricket-anova}
# Fit a reduced model:
main_effect_fit <-  lm(rate ~ temp + species, data = crickets) 

# Compare the two:
anova(main_effect_fit, interaction_fit)
```

This statistical test generates a p-value of `r format.pval(anova(interaction_fit, main_effect_fit)[2,6])`. This implies that there is a lack of evidence for the alternative hypothesis that the interaction term is needed by the model. For this reason, we will conduct further analysis on the model without the interaction. 

Residual plots should be re-assessed to make sure that our theoretical assumptions are valid enough to trust the p-values produced by the model (plots not shown here but spoiler alert: they are). 

We can use the `summary()` method to inspect the coefficients, standard errors, and p-values of each model term: 

```{r base-r-main-coef}
summary(main_effect_fit)
```

The chirp rate for each species increases by `r round(coef(main_effect_fit)[2], 2)` chirps as the temperature increases by a single degree. This term shows strong statistical significance as evidenced by the p-value.  The species term has a value of `r round(coef(main_effect_fit)[3], 2)`. This indicates that, across all temperature values, _O. niveus_ has a chirp rate that is about `r floor(abs(coef(main_effect_fit)[3]))` fewer chirps per minute than _O. exclamationis_. Similar to the temperature term, the species effect is associated with a very small p-value.  

The only issue in this analysis is the intercept value. It indicates that at 0 C, there are negative chirps per minute for both species. While this doesn't make sense, the data only go as low as `r min(crickets$temp)` C and interpreting the model at 0 C would be an _extrapolation_. This would be a bad idea. That being said, the model fit is good within the _applicable range_ of the temperature values; the conclusions should be limited to the observed temperature range. 

If we needed to estimate the chirp rate at a temperature that was not observed in the experiment, we could use the `predict()` method. It takes the model object and a data frame of new values for prediction. For example, the model estimates the chirp rate for _O. exclamationis_ for temperatures between 15 C and 20 C can be computed via:

```{r base-r-cricket-pred}
new_values <- data.frame(species = "O. exclamationis", temp = 15:20)
predict(main_effect_fit, new_values)
```

:::rmdwarning
Note that the non-numeric value of `species` is passed to the predict method, as opposed to the numeric, binary indicator variable.  
:::

While this analysis has obviously not been an exhaustive demonstration of R's modeling capabilities, it does highlight some major features important for the rest of this book: 

 * The language has an expressive syntax for specifying model terms for both simple and quite complex models.

 * The R formula method has many conveniences for modeling that are also applied to new data when predictions are generated. 

 * There are numerous helper functions (e.g., `anova()`, `summary()` and `predict()`) that you can use to conduct specific calculations after the fitted model is created. 

Finally, as previously mentioned, this framework was first published in 1992. Most of the ideas and methods above were developed in that period but have remained remarkably relevant to this day. It highlights that the S language and, by extension R, has been designed for data analysis since its inception.  

## What does the R formula do? {#formula}

The R model formula is used by many modeling packages. It usually serves multiple purposes: 

 * The formula defines the columns that are used by the model.  

 * The standard R machinery uses the formula to encode the columns into an appropriate format. 

 * The roles of the columns are defined by the formula. 

For the most part, practitioners' conception of what the formula does is dominated by the last purpose. Our focus when typing out a formula is often to declare how the columns should be used. For example, the previous specification we discussed sets up predictors to be used in a specific way: 

```r
(temp + species)^2
```

Our focus, when seeing this, is that there are two predictors and the model should contain their main effects and the two-way interactions.  However, this formula also implies that, since `species` is a factor, it should also create indicator variable columns for this predictor (see Section \@ref(dummies)) and multiply those columns by the `temp` column to create the interactions. This transformation represents the second bullet point above; the formula also defines _how each column is encoded_ and can create additional columns that are not in the original data. 

:::rmdwarning
This is an important point which will come up multiple times in this text, especially when we discuss more complex feature engineering in Chapter \@ref(recipes) and beyond. The formula in R has some limitations and our approaches to overcoming them contend with all three aspects listed above. 
:::

## Why tidiness is important for modeling {#tidiness-modeling}

One of the strengths of R is that it encourages developers to create a user-interface that fits their needs.  As an example, here are three common methods for creating a scatter plot of two numeric variables in a data frame called `plot_data`:

```{r base-r-three-plots, eval = FALSE}
plot(plot_data$x, plot_data$y)

library(lattice)
xyplot(y ~ x, data = plot_data)

library(ggplot2)
ggplot(plot_data, aes(x = x, y = y)) + geom_point()
```

In these three cases, separate groups of developers devised three distinct interfaces for the same task. Each has advantages and disadvantages. 

In comparison, the _Python Developer's Guide_ espouses the notion that, when approaching a problem:

> "There should be one -- and preferably only one -- obvious way to do it."

R is quite different from Python in this respect. An advantage of R's diversity of interfaces is that it can evolve over time and fit different types of needs for different users. 

Unfortunately, some of the syntactical diversity is due to a focus on the needs of the person developing the code instead of the needs of the person using the code. Inconsistencies between packages can be a stumbling block to R users. 

Suppose your modeling project has an outcome with two classes. There are a variety of statistical and machine learning models you could choose from. In order to produce a class probability estimate for each sample, it is common for a model function to have a corresponding `predict()` method. However, there is significant heterogeneity in the argument values used by those methods to make class probability predictions; this heterogeneity can be difficult for even experienced users to navigate. A sampling of these argument values for different models is: 

| Function     | Package                             | Code                                          |
| :----------- | :---------------------------------- | :-------------------------------------------- |
| `lda`        | <span class="pkg">MASS</span>       | `predict(object)`                             |
| `glm`        | <span class="pkg">stats</span>      | `predict(object, type = "response")`          |
| `gbm`        | <span class="pkg">gbm</span>        | `predict(object, type = "response", n.trees)` |
| `mda`        | <span class="pkg">mda</span>        | `predict(object, type = "posterior")`         |
| `rpart`      | <span class="pkg">rpart</span>      | `predict(object, type = "prob")`              |
| various      | <span class="pkg">RWeka</span>      | `predict(object, type = "probability")`       |
| `logitboost` | <span class="pkg">LogitBoost</span> | `predict(object, type = "raw", nIter)`        |
| `pamr.train` | <span class="pkg">pamr</span>       | `pamr.predict(object, type = "posterior")`    |

Note that the last example has a custom _function_ to make predictions instead of using the more common `predict()` interface (the generic `predict()` _method_). This lack of consistency is a barrier to day-to-day usage of R for modeling.

As another example of unpredictability, the R language has conventions for missing data which are handled inconsistently. The general rule is that missing data propagate more missing data; the average of a set of values with a missing data point is itself missing and so on. When models make predictions, the vast majority require all of the predictors to have complete values. There are several options baked in to R at this point with the generic function `na.action()`.  This sets the policy for how a function should behave if there are missing values. The two most common policies are `na.fail()` and `na.omit()`. The former produces an error if missing data are present while the latter removes the missing data prior to calculations by case-wise deletion. From our previous example:

```{r base-r-lm-missing, error = TRUE}
# Add a missing value to the prediction set
new_values$temp[1] <- NA

# The predict method for `lm` defaults to `na.pass`:
predict(main_effect_fit, new_values)

# Alternatively 
predict(main_effect_fit, new_values, na.action = na.fail)

predict(main_effect_fit, new_values, na.action = na.omit)
```

From a user's point of view, `na.omit()` can be problematic. In our example, `new_values` has `r nrow(new_values)` rows but only `r nrow(new_values) - 1` would be returned with `na.omit()`. To adjust for this, the user would have to determine which row had the missing value and interleave a missing value in the appropriate place if the predictions were merged into `new_values`^[A base R policy called `na.exclude()` does exactly this.]. While it is rare that a prediction function uses `na.omit()` as its missing data policy, this does occur. Users who have determined this as the cause of an error in their code find it _quite memorable_. 

To resolve the usage issues described here, the tidymodels packages have a set of design goals. Most of the tidymodels design goals fall under the existing rubric of **Design for Humans** from the tidyverse [@tidyverse], but with specific applications for modeling code. There are a few additional design goals that complement those of the tidyverse. Some examples: 

* R has excellent capabilities for _object oriented programming_ and we use this in lieu of creating new function names (such as a hypothetical new `predict_samples()` function). 

* _Sensible defaults_ are very important. Also, functions should have no default for arguments when it is more appropriate to force the user to make a choice (e.g., the file name argument for `read_csv()`).

* Similarly, argument values whose default _can_ be derived from the data should be. For example, for `glm()` the `family` argument could check the type of data in the outcome and, if no `family` was given, a default could be determined internally.

* Functions should take the **data structures that users have** as opposed to the data structure that developers want. For example, a model function's _only_ interface should not be constrained to matrices. Frequently, users will have non-numeric predictors such as factors. 

Many of these ideas are described in the tidymodels guidelines for model implementation^[https://tidymodels.github.io/model-implementation-principles]. In subsequent chapters, we will illustrate examples of existing issues, along with their solutions. 

:::rmdnote
There are a few existing R packages that provide a unified interface to harmonize these heterogeneous modeling APIs, such as `r pkg(caret)` and `r pkg(mlr)`. The tidymodels framework is similar to these in adopting a unification of the function interface, as well as enforcing consistency in the function names and return values. It is different in its opinionated design goals and modeling implementation.
:::

The `broom::tidy()` function, which we use throughout this book, is another tool for standardizing the structure of R objects. It can return many types of R objects in a more usable format. For example, suppose that predictors are being screened based on their correlation to the outcome column. Using `purrr::map()`, the results from `cor.test()` can be returned in a list for each predictor: 

```{r base-r-corr-list}
corr_res <- map(mtcars %>% select(-mpg), cor.test, y = mtcars$mpg)

# The first of ten results in the vector: 
corr_res[[1]]
```

If we want to use these results in a plot, the standard format of hypothesis test results are not very useful. The `tidy()` method can return this as a tibble with standardized names: 

```{r base-r-corr-tidy}
library(broom)

tidy(corr_res[[1]])
```

These results can be "stacked" and added to a `ggplot()`: 

```{r base-r-corr-plot}
corr_res %>% 
  # Convert each to a tidy format; `map_dfr()` stacks the data frames 
  map_dfr(tidy, .id = "predictor") %>% 
  ggplot(aes(x = fct_reorder(predictor, estimate))) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  labs(x = NULL, y = "Correlation with mpg")
```

Creating such a plot is possible using core R language functions, but automatically reformatting the results makes for more concise code with less potential for errors. 

## Combining base R models and the tidyverse

R modeling functions from the core language or other R packages can be used in conjunction with the tidyverse, especially with the `r pkg(dplyr)`, `r pkg(purrr)`, and `r pkg(tidyr)` packages. For example, if we wanted to fit separate models for each cricket species, we can first break out the cricket data by this column using `dplyr::group_nest()`: 

```{r base-r-by-species-split}
split_by_species <- 
  crickets %>% 
  group_nest(species) 
split_by_species
```

The `data` column contains the `rate` and `temp` columns from `crickets` in a _list column_. From this, the `purrr::map()` function can create individual models for each species:

```{r base-r-species-models}
model_by_species <- 
  split_by_species %>% 
  mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))
model_by_species
```

To collect the coefficients for each of these models, use `broom::tidy()` to convert them to a consistent data frame format so that they can be unnested:

```{r base-r-species-coefs}
model_by_species %>% 
  mutate(coef = map(model, tidy)) %>% 
  select(species, coef) %>% 
  unnest(cols = c(coef))
```

:::rmdnote
List columns can be very powerful in modeling projects. List columns provide containers for any type of R objects, from a fitted model itself to the important data frame structure. 
:::

## The tidymodels metapackage

The tidyverse (Chapter \@ref(tidyverse)) is designed as a set of modular R packages, each with a fairly narrow scope. The tidymodels framework follows a similar design. For example, the `r pkg(rsample)` package focuses on data splitting and resampling. Although resampling methods are critical to other activities of modeling (e.g., measuring performance), they reside in a single package and performance metrics are contained in a different, separate package, `r pkg(yardstick)`. There are many benefits to adopting this philosophy of modular packages, from less bloated model deployment to smoother package maintenance.

```{r base-r-detach, warning = FALSE, message = FALSE, echo = FALSE}
pkgs <- paste0("package:", c("tidymodels", tidymodels:::core))
for (i in pkgs) {
  try(detach(i, unload = TRUE, character.only = TRUE, force = TRUE), silent = TRUE)
}
```

The downside to this philosophy is that there are a lot of packages in the tidymodels organization. To compensate for this, the tidymodels _package_ (which you can think of as a "metapackage" like the tidyverse package) loads a core set of tidymodels and tidyverse packages. Loading the package shows which packages are attached and if there are function naming conflicts with previously loaded packages:

```{r base-r-tidymodels-package}
library(tidymodels)
```

As an example of a naming conflict, before loading `r pkg(tidymodels)`, invoking the `filter()` function will execute the function in the `r pkg(stats)` package. After loading tidymodels, it will execute the `r pkg(dplyr)` function of the same name. 

There are a few ways to handle naming conflicts. The function can be called with its namespace (e.g., `stats::filter()`). This is not bad practice but it does make the code less readable. 

Another option is to use the `r pkg(conflicted)` package. We can set a rule that remains in effect until the end of the R session to ensure that one specific function will always run if no namespace is given in the code. As an example, if we prefer the `r pkg(dplyr)` version of the above function:

```{r base-r-conflicted, eval = FALSE}
library(conflicted)
conflict_prefer("filter", winner = "dplyr")
```

For convenience, `r pkg(tidymodels)` contains a function that captures most of the common naming conflicts that we might encounter:

```{r base-r-clonflicts}
tidymodels_prefer(quiet = FALSE)
```

:::rmdwarning
Be aware that using this function opts you in to using `conflicted::conflict_prefer()` for _all_ namespace conflicts, making every conflict an error and forcing you to choose which function to use. The function `tidymodels::tidymodels_prefer()` handles the most common conflicts from tidymodels functions, but you will need to handle other conflicts in your R session yourself. 
:::

## Chapter summary

This chapter reviewed core R language conventions for creating and using models that are an important foundation for the rest of this book. The formula operator is an expressive and important aspect of fitting models in R and often serves multiple purposes in non-tidymodels functions. Traditional R approaches to modeling have some limitations, especially when it comes to fluently handling and visualizing model output. The `r pkg(tidymodels)` metapackage applies tidyverse design philosophy to modeling packages.
