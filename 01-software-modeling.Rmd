# 모델링을 위한 소프트웨어 {#software-modeling}

```{r software-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(gridExtra)
library(tibble)
library(kableExtra)

data(ames, package = "modeldata")
```


모델은 시스템을 설명하고 제공된 데이터의 관계를 캡처할 수 있는 수학적 도구입니다. 모델은 미래 이벤트 예측, 여러 그룹 간의 차이 여부 결정, 지도 기반 시각화 지원, 추가 조사가 가능한 데이터의 새로운 패턴 발견 등 다양한 목적으로 사용할 수 있습니다. 모델의 유용성은 _환원_ 능력에 달려 있습니다. 데이터의 주요 영향은 방정식으로 표현할 수 있는 관계와 같은 유용한 방식으로 수학적으로 캡처할 수 있습니다.  

21세기 초부터 수학적 모델은 명백하고 미묘한 방식으로 일상 생활에서 어디에나 존재하게 되었습니다. 많은 사람들의 일반적인 하루는 날씨를 확인하여 개를 산책시키기에 좋은 시간을 선택하고, 웹사이트에서 제품을 주문하고, 친구에게 문자 메시지를 입력하거나 자동으로 수정하고, 이메일을 확인하는 일이 포함될 수 있습니다. 이러한 각각의 경우에 어떤 유형의 모델이 관련되었을 가능성이 높습니다. 어떤 경우에는 모델의 사용을 쉽게 인식할 수 있지만('제품 X의 구매에 관심이 있을 수도 있음'), 그렇지 않은 경우도 있습니다(예: 스팸 이메일). 모델은 고객이 좋아할 만한 의복을 선택하고, 약물 후보로 평가될 분자를 식별하는 데 사용되거나, 심지어 사악한 회사가 과도하게 오염시키는 자동차의 발견을 피하기 위해 사용되는 메커니즘일 수도 있습니다. 좋든 나쁘든 모델은 계속 존재합니다.  

:::rmdnote
모델이 오늘날 우리 삶에 스며든 두 가지 이유가 있습니다. 모델을 생성하기 위한 풍부한 **소프트웨어**가 존재하고 **데이터**를 기록하고 접근하는 것이 더 쉬워졌습니다. 
:::

이 책은 주로 소프트웨어에 중점을 둡니다. 소프트웨어가 데이터를 표현하기 위해 정확한 관계를 생성하는 것은 매우 중요합니다. 대부분의 경우 수학적 정확성을 결정하는 것이 가능하지만 적절한 모델을 안정적으로 생성하려면 다음과 같은 것들이 더 필요합니다. 

첫째, 소프트웨어를 적절한 방식으로 쉽게 운용하는 것이 중요합니다. 사용자가 부적절하게 사용했다는 사실을 모를 정도로 사용자 인터페이스가 잘못 설계되어서는 안 됩니다. 예를 들어, Baggerly와 @baggerly2009는 세간의 이목을 끄는 컴퓨터 생물학 출판물의 데이터 분석에서 무수히 많은 문제를 보고했습니다. 문제 중 하나는 사용자가 모델 입력의 이름을 추가해야 하는 방법과 관련이 있었습니다. 소프트웨어의 사용자 인터페이스를 통해 실제 데이터 열에서 데이터의 열 이름을 쉽게 오프셋 할 수 있었습니다. 이로 인해 잘못된 유전자가 암 환자 치료에 중요한 것처럼 잘못 여겨졌고, 결국 여러 임상 시험이 종료되었습니다[@Carlson2012].

고품질 모델이 필요한 경우 소프트웨어를 적절하게 사용할 수 있어야 합니다. @abrams2003는 다음과 같은 흥미로운 원칙을 설명합니다:

> 성공의 중심: 정상에 오르기 위해서는 수많은 시행착오가 필요하지만, 우리가 고객에게 바라는 것은 우리의 플랫폼과 프레임워크를 사용하여 쉽게 원하는 목표를 달성하는 것입니다. 

데이터 분석 및 모델링 소프트웨어는 이 아이디어를 따라야 합니다. 

모델 구축의 두 번째 중요한 측면은 과학적 방법론과 관련이 있습니다. 복잡한 예측 모델로 작업할 때 논리적 오류 또는 부적절한 가정과 관련된 오류를 무의식적으로 범하기 쉽습니다. 많은 기계 학습 모델은 패턴 발견에 매우 능숙하여 나중에 재현되지 않는 데이터로부터 경험적 패턴을 쉽게 찾을 수 있습니다. 그러나, 이러한 유형의 방법론적 오류 중 일부는 실제 결과가 포함된 새 데이터를 얻을 때까지 문제가 감지되지 않을 수 있다는 점에서 조심해야 합니다.

:::rmdwarning
모델이 더욱 강력하고 복잡해짐에 따라 잠재적인 오류를 범하기도 더 쉬워집니다. 
:::

이 동일한 원칙은 프로그래밍에도 적용됩니다. 가능한 한 소프트웨어는 사용자가 **실수하지 않도록 보호**할 수 있어야 하고, **올바른 작업을 쉽게 수행**할 수 있도록 해야 합니다.

모델을 개발할 때는 이 두 가지 측면이 중요합니다. 모델을 만들기 위한 도구는 쉽게 구할 수 있고, 모델은 큰 영향을 미칠 수 있기 때문에 많은 사람들이 모델을 만들고 있습니다. 기술 전문성 및 교육 측면에서 그들의 배경은 다양합니다. 도구가 사용자의 경험에 견고해야 한다는 것이 중요합니다. 도구는 고성능 모델을 생성할 수 있을 만큼 강력해야 하면서도 적절한 방식으로 사용하기 쉬워야 합니다. 이 책은 이러한 특성을 염두에 두고 설계된 모델링용 소프트웨어 제품군에 대해 설명합니다.

여기에서 사용된 소프트웨어는 R 프로그래밍 언어를 기반으로 합니다[@baseR]. R은 데이터 분석 및 모델링을 위해 특별히 설계되었으며, 1970년대에 "아이디어들을 빠르고 충실하게 소프트웨어로 바꾸기"[@Chambers:1998] 위해 만들어진 S 언어(Scheme 과Lisp로부터 채택된 사전 범위 지정 규칙 포함)의 구현입니다.

R은 오픈 소스이며 무료입니다. 다양한 용도로 사용할 수 있는 강력한 프로그래밍 언어이지만 데이터 분석, 모델링, 시각화 및 기계 학습에 특화되어 있습니다. R은 쉽게 확장할 수 있으며, 모델링, 시각화 등과 같은 특정 주제에 중점을 둔 대부분의 사용자 기여 모듈인 방대한 패키지 생태계를 가지고 있습니다.

패키지 중에 ***tidyverse*** 라는 묶음이 있습니다[@tidyverse]. tidyverse는 데이터 과학을 위해 설계된 R 패키지의 독창적인 모음입니다. 모든 패키지는 기본 디자인 철학, 문법 및 데이터 구조를 공유합니다. 이러한 설계 철학 중 일부는 이 섹션에서 설명하는 소프트웨어 측면에서 직접적으로 영향을 받습니다. tidyverse 패키지를 사용한 적이 없다면 \@ref(tidyverse)장에 기본 개념에 대한 검토가 포함되어 있습니다. tidyverse 내에서 특별히 모델링에 초점을 맞춘 패키지의 하위 집합을 ***tidymodels*** 패키지라고 합니다. 이 책은 tidyverse와 tidymodels를 사용하여 모델링을 수행하기 위한 확장된 소프트웨어 매뉴얼입니다. 각각 고유한 특정 목적을 가진 패키지 세트를 사용하여 고품질 모델을 만드는 방법을 보여줍니다.

## 모델 유형 {#model-types}

계속하기 전에 목적별로 그룹화된 모델 유형에 대한 분류를 설명하겠습니다. 완전하지는 않지만 대부분의 모델은 다음 범주 중 하나 이상에 속합니다:

### 서술적 모델 {-}

서술적 모델의 목적은 데이터의 특성을 설명하거나 서술하는 것입니다. 서술적 분석은 데이터의 일부 추세 또는 인공물을 시각적으로 강조하는 것이 주 목적일 수 있습니다.

예를 들어, 한동안 마이크로어레이를 사용하여 RNA의 대규모 측정이 가능했습니다. 초기 실험실에서 사용된 방법은 생물학적 샘플을 작은 마이크로칩에 넣었습니다. 칩의 매우 작은 위치는 특정 RNA 시퀀스의 풍부함을 기반으로 신호를 측정할 수 있습니다. 칩에는 수천(또는 그 이상)의 결과가 포함될 것이며, 각각은 일부 생물학적 과정과 관련된 RNA의 정량화입니다. 그러나 칩의 품질에 문제가 있어서 결과가 좋지 않을 수 있습니다. 실수로 칩의 일부에 지문이 남아 있으면 스캔 시 부정확한 측정이 발생할 수 있습니다.

이러한 문제를 평가하는 초기 방법은 _프로브 수준 모델_ 또는 PLM이었습니다[@bolstad2004]. 이 모델을 통해 칩, RNA 서열, 서열 유형 등과 같은 데이터의 _알려진_ 차이점을 설명하는 통계 모델이 생성됩니다. 데이터에 알 수 없는 다른 요인이 있는 경우 이러한 효과는 모델 잔차에 캡처됩니다. 잔차가 칩의 위치에 따라 표시되는 반면 좋은 품질의 칩은 패턴을 표시하지 않습니다. 문제가 발생하면 일종의 공간 패턴을 식별할 수 있습니다. 종종 패턴 유형은 근본적인 문제(예: 지문)와 가능한 해결책(칩을 닦아내고 다시 스캔, 샘플 반복 등)을 제안합니다. 그림 1.1(a)는 Gentleman et al. (2005)에서 가져온 두 개의 마이크로어레이에 이 방법을 적용한 것을 보여줍니다. 이미지는 두 가지 다른 색상을 보여줍니다. 빨간색은 신호 강도가 모델이 예상한 것보다 큰 반면 파란색은 예상보다 낮은 값을 나타냅니다. 왼쪽 패널은 상당히 무작위적인 패턴을 보여주는 반면 오른쪽 패널은 칩 중앙에 바람직하지 않은 인공물을 나타냅니다.

An early method for evaluating such issues were _probe-level models_, or PLM's [@bolstad2004]. A statistical model would be created that accounted for the _known_ differences in the data, such as the chip, the RNA sequence, the type of sequence, and so on. If there were other, unknown factors in the data, these effects would be captured in the model residuals. When the residuals were plotted by their location on the chip, a good quality chip would show no patterns. When a problem did occur, some sort of spatial pattern would be discernible. Often the type of pattern would suggest the underlying issue (e.g. a fingerprint) and a possible solution (wipe the chip off and rescan, repeat the sample, etc.). Figure \@ref(fig:software-descr-examples)(a) shows an application of this method for two microarrays taken from @Gentleman2005. The images show two different colors; red is where the signal intensity was larger than the model expects while the blue color shows lower than expected values. The left-hand panel demonstrates a fairly random pattern while the right-hand panel exhibits an undesirable artifact in the middle of the chip. 

```{r software-descr-examples, echo = FALSE, fig.cap = "Two examples of how descriptive models can be used to illustrate specific patterns.", out.width = '80%', dev = "png", fig.height = 8, warning = FALSE, message = FALSE}
load("RData/plm_resids.RData")

resid_cols <- RColorBrewer::brewer.pal(8, "Set1")[1:2]

# Red is where intensity is higher than expected
plm_plot <- 
  plm_resids %>% 
  mutate(sign = ifelse(Intensity < 0, "low", "high")) %>% 
  ggplot(aes(x = x, y = y, fill = sign))  + 
  geom_tile(show.legend = FALSE) + 
  facet_wrap(~Sample) + 
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank()
  ) + 
  labs(x = "", y = "") + 
  scale_fill_manual(values = c("#377EB8", "#E41A1C")) + 
  coord_equal() + 
  ggtitle("(a) Evaluating the quality of two microarray chips using a model.") + 
  theme(plot.title = element_text(hjust = 0.5))


ames_plot <- 
  ggplot(ames, aes(x = Latitude, y = Sale_Price)) + 
  geom_point(alpha = .2) + 
  geom_smooth(se = FALSE, method = stats::loess, method.args = list(span = .3), col = "red") + 
  scale_y_log10() + 
  ylab("House Sale Price ($US)") + 
  ggtitle("(b) Using a model-based smoother to discover trends.")

grid.arrange(plm_plot, ames_plot, ncol = 1)
```

Another example of a descriptive model is the _locally estimated scatterplot smoothing_ model, more commonly known as LOESS [@cleveland1979]. Here, a smooth and flexible regression model is fit to a data set, usually with a single independent variable, and the fitted regression line is used to elucidate some trend in the data. These types of smoothers are used to discover potential ways to represent a variable in a model. This is demonstrated in Figure \@ref(fig:software-descr-examples)(b) where a nonlinear trend is illuminated by the flexible smoother. From this plot, it is clear that there is a highly nonlinear relationship between the sale price of a house and its latitude. 


### Inferential models {-}

The goal of an inferential model is to produce a decision for a research question or to test a specific hypothesis, in much the way that statistical tests are used^[Many specific statistical tests are in fact equivalent to models. For example, t-tests and analysis of variance (ANOVA) methods are particular cases of the generalized linear model.]. The goal is to make some statement of truth regarding a predefined conjecture or idea. In many (but not all) cases, a qualitative statement is produced (e.g., a difference was "statistically significant").

For example, the goal of a clinical trial might be to provide confirmation that a new therapy does a better job in prolonging life than an alternative, like an existing therapy or no treatment at all. If the clinical endpoint was related to survival of a patient, the _null hypothesis_ might be that the two therapeutic groups have equal median survival times, with the _alternative hypothesis_ being that the new therapy has higher median survival.  If this trial were evaluated using traditional null hypothesis significance testing via modeling, the significance testing would produce a p-value using some pre-defined methodology based on a set of assumptions for the data. Small values for the p-value in the model results would indicate that there is evidence that the new therapy helps patients live longer. Large values for the p-value in the model results would conclude that there is a failure to show such a difference; this lack of evidence could be due to a number of reasons, including the therapy not working. 

What are the important aspects of this type of analysis? Inferential modeling techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. Generally, to compute such a quantity, formal probabilistic assumptions must be made about the data and the underlying processes that generated the data. The quality of the statistical modeling results are highly dependent on these pre-defined assumptions as well as how much the observed data appear to agree with them. The most critical factors here are theoretical in nature: "If my data were independent and follow distribution _X_, then test statistic _Y_ can be used to produce a p-value. Otherwise, the resulting p-value might be inaccurate."

One aspect of inferential analyses is that there tends to be a delayed feedback loop in understanding how well the data matches the model assumptions. In our clinical trial example, if statistical (and clinical) significance indicate that the new therapy should be available for patients to use, it still may be years before it is used in the field and enough data are generated for an independent assessment of whether the original statistical analysis led to the appropriate decision. 


### Predictive models {-}

Sometimes data are modeled to produce the most accurate prediction possible for new data. Here, the primary goal is that the predicted values have the highest possible fidelity to the true value of the new data. 

A simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month. An over-prediction wastes space and money due to excess books. If the prediction is smaller than it should be, there is opportunity loss and less profit. 

For this type of model, the problem type is one of _estimation_ rather than inference. For example, the buyer is usually not concerned with a question such as "Will I sell more than 100 copies of book _X_ next month?" but rather "How many copies of book _X_ will customers purchase next month?" Also, depending on the context, there may not be any interest in _why_ the predicted value is _X_. In other words, there is more interest in the value itself than evaluating a formal hypothesis related to the data. The prediction can also include measures of uncertainty. In the case of the book buyer, providing a forecasting error may be helpful in deciding how many to purchase. It can also serve as a metric to gauge how well the prediction method worked.  

What are the most important factors affecting predictive models? There are many different ways that a predictive model can be created, so the important factors depend on how the model was developed.

A **mechanistic model** could be derived using first principles to produce a model equation that is dependent on assumptions. For example, when predicting the amount of a drug that is in a person's body at a certain time, some formal assumptions are made on how the drug is administered, absorbed, metabolized, and eliminated. Based on this, a set of differential equations can be used to derive a specific model equation. Data are used to estimate the unknown parameters of this equation so that predictions can be generated. Like inferential models,  mechanistic predictive models greatly depend on the assumptions that define their model equations. However, unlike inferential models, it is easy to make data-driven statements about how well the model performs based on how well it predicts the existing data. Here the feedback loop for the modeling practitioner is much faster than it would be for a hypothesis test. 

**Empirically driven models** are created with more vague assumptions. These models tend to fall into the machine learning category. A good example is the _K_-nearest neighbor (KNN) model. Given a set of reference data, a new sample is predicted by using the values of the _K_ most similar data in the reference set. For example, if a book buyer needs a prediction for a new book, historical data from existing books may be available. A 5-nearest neighbor model would estimate the amount of the new books to purchase based on the sales numbers of the five books that are most similar to the new one (for some definition of "similar"). This model is only defined by the structure of the prediction (the average of five similar books). No theoretical or probabilistic assumptions are made about the sales numbers or the variables that are used to define similarity. In fact, the primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data. If the structure of this type of model was a good choice, the predictions would be close to the actual values. 

Broader discussions of these distinctions can be found in @breiman2001 and @shmueli2010. 

:::rmdnote
Note that we have defined the type of a model by how it is used, rather than its mathematical qualities. 
:::

An ordinary linear regression model might fall into any of these three classes of model, depending on how it is used: 

* A descriptive smoother, similar to LOESS, called _restricted smoothing splines_ [@Durrleman1989] can be used to describe trends in data using ordinary linear regression with specialized terms. 

* An _analysis of variance_ (ANOVA) model is a popular method for producing the p-values used for inference. ANOVA models are a special case of linear regression. 

* If a simple linear regression model produces highly accurate predictions, it can be used as a predictive model. 

There are many examples of predictive models that cannot (or at least should not) be used for inference. Even if probabilistic assumptions were made for the data, the nature of the KNN model makes the math required for inference intractable. 

There is an additional connection between the types of models. While the primary purpose of descriptive and inferential models might not be related to prediction, the predictive capacity of the model should not be ignored. For example, logistic regression is a popular model for data where the outcome is qualitative with two possible values. It can model how variables are related to the probability of the outcomes. When used in an inferential manner, there is usually an abundance of attention paid to the _statistical qualities_ of the model. For example, analysts tend to strongly focus on the selection of which independent variables are contained in the model. Many iterations of model building are usually used to determine a minimal subset of independent variables that have a  "statistically significant" relationship to the outcome variable. This is usually achieved when all of the p-values for the independent variables are below some value (e.g. 0.05). From here, the analyst typically focuses on making qualitative statements about the relative influence that the variables have on the outcome (e.g., "There is a statistically significant relationship between age and the odds of heart disease.").  

This can be dangerous when statistical significance is used as the _only_ measure of model quality.  It is possible that this statistically optimized model has poor model accuracy, or performs poorly on some other measure of predictive capacity. While the model might not be used for prediction, how much should inferences be trusted from a model that has significant p-values but dismal accuracy? Predictive performance tends to be related to how close the model's fitted values are to the observed data. 

:::rmdwarning
If a model has limited fidelity to the data, the inferences generated by the model should be highly suspect. In other words, statistical significance may not be sufficient proof that a model is appropriate. 
:::

This may seem intuitively obvious, but is often ignored in real-world data analysis.

## Some terminology {#model-terminology}

Before proceeding, we outline here some additional terminology related to modeling and data. These descriptions are intended to be helpful as you read this book but not exhaustive. 

First, many models can be categorized as being _supervised_ or _unsupervised_. Unsupervised models are those that learn patterns, clusters, or other characteristics of the data but lack an outcome, i.e., a dependent variable. Principal component analysis (PCA), clustering, and autoencoders are examples of unsupervised models; they are used to understand relationships between variables or sets of variables without an explicit relationship between predictors and an outcome. Supervised models are those that have an outcome variable. Linear regression, neural networks, and numerous other methodologies fall into this category. 

Within supervised models, there are two main sub-categories: 

 * **Regression** predicts a numeric outcome.

 * **Classification** predicts an outcome that is an ordered or unordered set of qualitative values.  

These are imperfect definitions and do not account for all possible types of models. In Chapter \@ref(models), we refer to this characteristic of supervised techniques as the _model mode_. 

Different variables can have different _roles_, especially in a supervised modeling analysis. Outcomes (otherwise known as the labels, endpoints, or dependent variables) are the value being predicted in supervised models. The independent variables, which are the substrate for making predictions of the outcome, are also referred to as predictors, features, or covariates (depending on the context). The terms _outcomes_ and _predictors_ are used most frequently in this book. 

In terms of the data or variables themselves, whether used for supervised or unsupervised models, as predictors or outcomes, the two main categories are quantitative and qualitative. Examples of the former are real numbers like `3.14159` and integers like `42`. Qualitative values, also known as nominal data, are those that represent some sort of discrete state that cannot be naturally placed on a numeric scale, like "red", "green", and "blue". 


## How does modeling fit into the data analysis process? {#model-phases}

In what circumstances are models created? Are there steps that precede such an undertaking? Is it the first step in data analysis? 

:::rmdnote
There are always a few critical phases of data analysis that come before modeling. 
:::

First, there is the chronically underestimated process of **cleaning the data**. No matter the circumstances, you should investigate the data to make sure that they are applicable to your project goals, accurate, and appropriate. These steps can easily take more time than the rest of the data analysis process (depending on the circumstances). 

Data cleaning can also overlap with the second phase of **understanding the data**, often referred to as exploratory data analysis (EDA). EDA brings to light how the different variables are related to one another, their distributions, typical ranges, and other attributes. A good question to ask at this phase is, "How did I come by _these_ data?" This question can help you understand how the data at hand have been sampled or filtered and if these operations were appropriate. For example, when merging database tables, a join may go awry that could accidentally eliminate one or more sub-populations. Another good idea is to ask if the data are _relevant_. For example, to predict whether patients have Alzheimer's disease or not, it would be unwise to have a data set containing subjects with the disease and a random sample of healthy adults from the general population. Given the progressive nature of the disease, the model may simply predict who are the _oldest patients_. 

Finally, before starting a data analysis process, there should be clear expectations of the goal of the model and how performance (and success) will be judged. At least one _performance metric_ should be identified with realistic goals of what can be achieved. Common statistical metrics, discussed in more detail in Chapter \@ref(performance), are classification accuracy, true and false positive rates, root mean squared error, and so on. The relative benefits and drawbacks of these metrics should be weighed. It is also important that the metric be germane; alignment with the broader data analysis goals is critical. 

```{r software-data-science-model, echo = FALSE, out.width = '80%', fig.cap = "The data science process (from R for Data Science).", warning = FALSE}
knitr::include_graphics("premade/data-science-model.svg")
```

The process of investigating the data may not be simple. @wickham2016 contains an excellent illustration of the general data analysis process, reproduced with Figure \@ref(fig:software-data-science-model). Data ingestion and cleaning/tidying are shown as the initial steps. When the analytical steps for understanding commence, they are a heuristic process; we cannot pre-determine how long they may take. The cycle of analysis, modeling, and visualization often requires multiple iterations. 

```{r software-modeling-process, echo = FALSE, out.width = '100%', fig.width=8, fig.height=3, fig.cap = "A schematic for the typical modeling process.", warning = FALSE}
knitr::include_graphics("premade/modeling-process.svg")
```

This iterative process is especially true for modeling. Figure \@ref(fig:software-modeling-process) is meant to emulate the typical path to determining an appropriate model. The general phases are:

 * **Exploratory data analysis (EDA):** Initially there is a back and forth between numerical analysis and visualization of the data (represented in Figure \@ref(fig:software-data-science-model)) where different discoveries lead to more questions and data analysis "side-quests" to gain more understanding. 
 
 * **Feature engineering:** The understanding gained from EDA results in the creation of specific model terms that make it easier to accurately model the observed data. This can include complex methodologies (e.g., PCA) or simpler features (using the ratio of two predictors). Chapter \@ref(recipes) focuses entirely on this important step.

 * **Model tuning and selection (circles with blue and yellow segments):** A variety of models are generated and their performance is compared. Some models require _parameter tuning_ where some structural parameters are required to be specified or optimized. The colored segments within the circles signify the repeated data splitting used during resampling (see Chapter \@ref(resampling)). 
 
* **Model evaluation:** During this phase of model development, we assess the model's performance metrics, examine residual plots, and conduct other EDA-like analyses to understand how well the models work. In some cases, formal between-model comparisons (Chapter \@ref(compare)) help you to understand whether any differences in models are within the experimental noise.   

After an initial sequence of these tasks, more understanding is gained regarding which types of models are superior as well as which sub-populations of the data are not being effectively estimated. This leads to additional EDA and feature engineering, another round of modeling, and so on. Once the data analysis goals are achieved, the last steps are typically to finalize, document, and communicate the model. For predictive models, it is common at the end to validate the model on an additional set of data reserved for this specific purpose. 

As an example, @fes use data to model the daily ridership of Chicago's public train system using predictors such as the date, the previous ridership results, the weather, and other factors. An approximation of these authors' "inner monologue" when analyzing these data is, in order:

```{r software-monolog, echo = FALSE, results = 'as-is'}
monolog <- 
  tribble(
    ~Activity, ~`Analysis Cycle`, ~Thoughts,
    "EDA", "1",
    "The daily ridership values between stations are extremely correlated.",
    "EDA", " ",
    "Weekday and weekend ridership look very different.",
    "EDA", " ",
    "One day in the summer of 2010 has an abnormally large number of riders.",
    "EDA", "1",
    "Which stations had the lowest daily ridership values?",
    "Feature Engineering", "1",
    "Dates should at least be encoded as day-of-the-week, and year. ",
    "Feature Engineering", " ",
    "Maybe PCA could be used on the correlated predictors to make it easier for the models to use them. ",
    "Feature Engineering", " ",
    "Hourly weather records should probably be summarized into daily measurements. ",
    "Model Fitting", "1",
    "Let’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree. ",
    "Model Tuning", "1",
    "How many neighbors should be used?",
    "Model Tuning", " ",
    "Should we run a lot of boosting iterations or just a few?",
    "Model Tuning", "2",
    "How many neighbors seemed to be optimal for these data? ",
    "Model Evaluation", "2",
    "Which models have the lowest root mean squared errors? ",
    "EDA", "2",
    "Which days were poorly predicted? ",
    "Model Evaluation", "2",
    "Variable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models. ",
    "Model Evaluation", " ",
    "It seems like we should focus on a lot of boosting iterations for that model.",
    "Feature Engineering", "2", 
    "We need to encode holiday features to improve predictions on (and around) those dates.",
    "Model Evaluation", "2",
    "Let’s drop K-NN from the model list. "
  )
if (knitr::is_html_output()) {
  tab <- 
    monolog %>% 
    dplyr::select(Thoughts, Activity) %>% 
    kable() %>%
    kable_styling() %>% 
    column_spec(2, width = "25%") %>%
    column_spec(1, width = "75%", italic = TRUE)
} else {
  tab <- 
    monolog %>% 
    dplyr::select(Thoughts, Activity) %>% 
    kable() %>%
    kable_styling()
}
tab
```

and so on. Eventually, a model is selected that is able to achieve sufficient performance.

## Chapter summary {#software-summary}

This chapter focused on how models describe relationships in data, and different types of models such as descriptive models, inferential models, and predictive models. The predictive capacity of a model can be used to evaluate it, even when its main goal is not prediction. Modeling itself sits within the broader data analysis process, and exploratory data analysis is a key part of building high-quality models.

For all kinds of modeling, software for building models must support good scientific methodology and ease of use for practitioners from diverse backgrounds. The software we develop approaches this with the ideas and syntax of the tidyverse, which we introduce (or review) in Chapter \@ref(tidyverse). Chapter \@ref(base-r) is a quick tour of conventional base R modeling functions and summarize the unmet needs in that area. 

After that, this book is separated into parts, starting with the basics of modeling with tidy data principles. The first part introduces an example data set on house prices and demonstrates how to use the fundamental tidymodels packages: `r pkg(recipes)`, `r pkg(parsnip)`, `r pkg(workflows)`, `r pkg(yardstick)`, and others. 

The second part of the book moves forward with more details on the process of creating a good model. This includes creating good estimates of performance as well as tuning model parameters. 


